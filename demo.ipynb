{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI API\n",
    "\n",
    "Welcome to this demonstration of the OpenAI API! OpenAI offers one of the most advanced natural language processing (NLP) models, capable of understanding and generating human-like text based on the prompts given to it. The OpenAI API provides developers with a way to interact with these models and integrate their capabilities into various applications.\n",
    "\n",
    "## Objective of this Demonstration\n",
    "\n",
    "The primary goal of this notebook is to introduce you to the OpenAI API and demonstrate how to interact with it using Python. We will cover the basics of setting up the API, crafting prompts, handling responses, and exploring advanced features. By the end of this demonstration, you should have a solid understanding of how to leverage the OpenAI API for various NLP tasks.\n",
    "\n",
    "## What We Will Cover\n",
    "\n",
    "- **Setup and Installation:** Installing the necessary libraries and setting up the API key.\n",
    "- **Authentication:** Authenticating with the OpenAI API.\n",
    "- **Basic Usage:** A simple example to get started with using the API.\n",
    "- **Prompt Engineering:** Crafting effective prompts to get desired responses.\n",
    "- **Handling Responses:** Parsing and extracting information from the API responses.\n",
    "- **Advanced Usage:** Exploring more advanced features and usage of the API.\n",
    "- **Best Practices:** Tips and best practices for using the OpenAI API.\n",
    "\n",
    "Let's dive in and explore the capabilities of this powerful tool!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Installation\n",
    "\n",
    "Before we start interacting with the OpenAI API, we need to ensure that we have the `openai` Python package installed, along with `python-dotenv` for securely managing our API key.\n",
    "\n",
    "## 1.1 Installing the Required Packages\n",
    "\n",
    "To install the `openai` and `python-dotenv` packages, you can use the `pip` command. `pip` is a package manager for Python that simplifies the process of managing and installing Python libraries. Run the following cell to install the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (1.47.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Users/madelynswelstad/miniforge3/envs/ml/lib/python3.11/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai \n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 API Key Setup\n",
    "\n",
    "After installing the required packages, the next step is to set up the API key. The API key is a unique identifier that authenticates your requests to the OpenAI API. \n",
    "\n",
    "1. **Obtain API Key:** If you don't have an API key yet, you can obtain one by signing up on the [OpenAI Platform](https://beta.openai.com/signup/). For our class group project, your project leader should obtain an API key, then share it with all group members.\n",
    "\n",
    "2. **Store API Key Securely:** For security reasons, it is recommended to store the API key in a `.env` file and not to hardcode it directly in the notebook. Create a `.env` file in the same directory as your notebook and add the following line to it:\n",
    "\n",
    "    `OPENAI_API_KEY=your_api_key_here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic Usage\n",
    "\n",
    "Now that we have set up our environment and loaded our API key, let's explore the basic usage of the OpenAI API. We will define a function `get_completion` that takes a prompt and a model name as input and returns the model's completion. In LLM, we usually call the user message '*context*', and the message sent by LLM '*completion*'.\n",
    "\n",
    "## 2.1 Defining the Function\n",
    "\n",
    "We will define a function named `get_completion` which will take two parameters:\n",
    "- `prompt`: The text prompt that we want to complete.\n",
    "- `model`: The name of the model we want to use (default is \"gpt-3.5-turbo\").\n",
    "\n",
    "The function will send these parameters to the OpenAI API and return the completion generated by the model. We will set the `temperature` parameter to 0, which means the output will be deterministic and less random.\n",
    "\n",
    "This function will use OpenAI's `gpt-4o-mini` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat). So this fuction is intended to complete a single-turn task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output; 0 <= temperature <= 2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Testing the Function\n",
    "\n",
    "Let's test our `get_completion` function by providing a simple prompt and observing the model's completion. We will use the default model \"gpt-3.5-turbo\" for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment vas-tu ?\n"
     ]
    }
   ],
   "source": [
    "# Define a test prompt\n",
    "test_prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Call the get_completion function with the test prompt\n",
    "response = get_completion(test_prompt)\n",
    "\n",
    "# Display the completion\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we provided a prompt asking the model to translate a sentence from English to French. The `get_completion` function sends this prompt to the OpenAI API and prints the model's completion, which should be the translated sentence in French.\n",
    "\n",
    "Feel free to experiment with different prompts and observe how the model responds. Keep in mind the capabilities and limitations of the model, and explore how different prompts can yield different results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prompt Engineering\n",
    "\n",
    "Prompt engineering is a crucial aspect of working with language models like GPT-3.5 Turbo. It involves crafting effective prompts that guide the model to generate the desired output. A well-crafted prompt can significantly improve the model's performance and the quality of its responses.\n",
    "\n",
    "## 3.1 Importance of Prompt Engineering\n",
    "\n",
    "- **Guidance:** A clear and concise prompt guides the model towards generating relevant and accurate responses.\n",
    "- **Context:** Providing sufficient context helps the model understand the task better.\n",
    "- **Instruction:** Explicit instructions can be used to specify the format or structure of the desired output.\n",
    "\n",
    "## 3.2 Prompting Principles\n",
    "- **Principle 1: Write clear and specific instructions**\n",
    "- **Principle 2: Give the model time to “think”**\n",
    "\n",
    "### 3.2.1 Principle 1: Write clear and specific instructions\n",
    "\n",
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\\\ \n",
    "providing instructions that are as clear and \\\\ \n",
    "specific as you can possibly make them. \\\\\n",
    "This will guide the model towards the desired output, \\\\\n",
    "and reduce the chances of receiving irrelevant \\\\\n",
    "or incorrect responses. Don't confuse writing a \\\\\n",
    "clear prompt with writing a short prompt. \\\\\n",
    "In many cases, longer prompts provide more clarity \\\\\n",
    "and context for the model, which can lead to \\\\\n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\\\n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 2: Ask for a structured output\n",
    "- JSON, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\\\\n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \\\\\n",
    "water boiling. While that's happening, \\\\\n",
    "grab a cup and put a tea bag in it. Once the water is \\\\\n",
    "hot enough, just pour it over the tea bag. \\\\\n",
    "Let it sit for a bit so the tea can steep. After a \\\\\n",
    "few minutes, take out the tea bag. If you \\\\\n",
    "like, you can add some sugar or milk to taste. \\\\\n",
    "And that's it! You've got yourself a delicious \\\\\n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\\\\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\\\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 4: \"Few-shot\" prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\\\\n",
    "valley flows from a modest spring; the \\\\\n",
    "grandest symphony originates from a single note; \\\\\n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Principle 2: Give the model time to “think” \n",
    "\n",
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \\\\\n",
    "a quest to fetch water from a hilltop \\\\\n",
    "well. As they climbed, singing joyfully, misfortune \\\\\n",
    "struck—Jack tripped on a stone and tumbled \\\\\n",
    "down the hill, with Jill following suit. \\\\\n",
    "Though slightly battered, the pair returned home to \\\\\n",
    "comforting embraces. Despite the mishap, \\\\\n",
    "their adventurous spirits remained undimmed, and they \\\\\n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\\\\n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "You must use the following format to answer the question:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\\\n",
    "me a flat $100k per year, and an additional $10 / square \\\\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Iterative Prompt Development\n",
    "\n",
    "We can use an iterative approach to solve some issues in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_sheet_chair = \"\"\"\n",
    "OVERVIEW\n",
    "- Part of a beautiful family of mid-century inspired office furniture, \n",
    "including filing cabinets, desks, bookcases, meeting tables, and more.\n",
    "- Several options of shell color and base finishes.\n",
    "- Available with plastic back and front upholstery (SWC-100) \n",
    "or full upholstery (SWC-110) in 10 fabric and 6 leather options.\n",
    "- Base finish options are: stainless steel, matte black, \n",
    "gloss white, or chrome.\n",
    "- Chair is available with or without armrests.\n",
    "- Suitable for home or business settings.\n",
    "- Qualified for contract use.\n",
    "\n",
    "CONSTRUCTION\n",
    "- 5-wheel plastic coated aluminum base.\n",
    "- Pneumatic chair adjust for easy raise/lower action.\n",
    "\n",
    "DIMENSIONS\n",
    "- WIDTH 53 CM | 20.87”\n",
    "- DEPTH 51 CM | 20.08”\n",
    "- HEIGHT 80 CM | 31.50”\n",
    "- SEAT HEIGHT 44 CM | 17.32”\n",
    "- SEAT DEPTH 41 CM | 16.14”\n",
    "\n",
    "OPTIONS\n",
    "- Soft or hard-floor caster options.\n",
    "- Two choices of seat foam densities: \n",
    " medium (1.8 lb/ft3) or high (2.8 lb/ft3)\n",
    "- Armless or 8 position PU armrests \n",
    "\n",
    "MATERIALS\n",
    "SHELL BASE GLIDER\n",
    "- Cast Aluminum with modified nylon PA6/PA66 coating.\n",
    "- Shell thickness: 10 mm.\n",
    "SEAT\n",
    "- HD36 foam\n",
    "\n",
    "COUNTRY OF ORIGIN\n",
    "- Italy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Issue 1: The text is too long \n",
    "- Limit the number of words/sentences/characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "Use at most 50 words.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Issue 2. Text focuses on the wrong details\n",
    "- Ask it to focus on the aspects that are relevant to the intended audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "The description is intended for furniture retailers, \n",
    "so should be technical in nature and focus on the \n",
    "materials the product is constructed from.\n",
    "\n",
    "Use at most 50 words.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Summarizing\n",
    "\n",
    "Summarize text with a focus on specific topics.\n",
    "\n",
    "### Text to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_review = \"\"\"\n",
    "Got this panda plush toy for my daughter's birthday, \\\\\n",
    "who loves it and takes it everywhere. It's soft and \\\\\n",
    "super cute, and its face has a friendly look. It's \\\\ \n",
    "a bit small for what I paid though. I think there \\\\\n",
    "might be other options that are bigger for the \\\\\n",
    "same price. It arrived a day earlier than expected, \\\\\n",
    "so I got to play with it myself before I gave it \\\\\n",
    "to her.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize with a word/sentence/character limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to generate a short summary of a product \\\\\n",
    "review from an e-commerce site. \n",
    "\n",
    "Summarize the review below, delimited by triple \n",
    "backticks, in at most 30 words. \n",
    "\n",
    "Review: ```{prod_review}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize with a focus on shipping and delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to generate a short summary of a product \\\\\n",
    "review from an e-commerce site to give feedback to the \\\\\n",
    "Shipping department. \n",
    "\n",
    "Summarize the review below, delimited by triple \n",
    "backticks, in at most 30 words, and focusing on any aspects \\\\\n",
    "that mention shipping and delivery of the product. \n",
    "\n",
    "Review: ```{prod_review}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize with a focus on price and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to generate a short summary of a product \\\\\n",
    "review from an ecommerce site to give feedback to the \\\\\n",
    "pricing department, responsible for determining the \\\\\n",
    "price of the product. \n",
    "\n",
    "Summarize the review below, delimited by triple \n",
    "backticks, in at most 30 words, and focusing on any aspects \\\\\n",
    "that are relevant to the price and perceived value. \n",
    "\n",
    "Review: ```{prod_review}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Chatbot\n",
    "\n",
    "In this section, you will explore how you can utilize the chat format to have extended conversations with chatbots personalized or specialized for specific tasks or behaviors.\n",
    "\n",
    "### 5.1 Setup\n",
    "\n",
    "In this setup, we utilize different roles in GPT to give more context to the LLM. The previous function `get_completion` only specify the `user` role. Here we use all three roles:\n",
    "- The `system` role gives a system level instruction to our chatbot.\n",
    "- The `user` role is the user (you).\n",
    "- The `assitant` role is the AI that gives you responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-4o\", temperature=0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "#     print(str(response.choices[0].message))\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "{'role':'user', 'content':'tell me a joke'},   \n",
    "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "{'role':'user', 'content':'I don\\'t know'}  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_from_messages(messages, temperature=1) # we use a higher temperature to get more creative responses\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Multi-message Conversation\n",
    "\n",
    "Now let's see how GPT handles context of conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the chatbot cannot answer your question, because every conversation is independent, the chatbot cannot get context from the previous conversation. So we need to combine those conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},\n",
    "{'role':'user', 'content':'Hi, my name is Isa'},\n",
    "#{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\\\n",
    "#Is there anything I can help you with today?\"},\n",
    "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 OrderBot\n",
    "We can automate the collection of user prompts and assistant responses to build a  OrderBot. The OrderBot will take orders at a pizza restaurant. \n",
    "\n",
    "We start from building a function to collect all messages. Then we build a simple GUI for the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = get_completion_from_messages(context) \n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600)))\n",
    " \n",
    "    return pn.Column(*panels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "\n",
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
    "You first greet the customer, then collects the order, \\\n",
    "and then asks if it's a pickup or delivery. \\\n",
    "You wait to collect the entire order, then summarize it and check for a final \\\n",
    "time if the customer wants to add anything else. \\\n",
    "If it's a delivery, you ask for an address. \\\n",
    "Finally you collect the payment.\\\n",
    "Make sure to clarify all options, extras and sizes to uniquely \\\n",
    "identify the item from the menu.\\\n",
    "You respond in a short, very conversational friendly style. \\\n",
    "The menu includes \\\n",
    "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
    "cheese pizza   10.95, 9.25, 6.50 \\\n",
    "eggplant pizza   11.95, 9.75, 6.75 \\\n",
    "fries 4.50, 3.50 \\\n",
    "greek salad 7.25 \\\n",
    "Toppings: \\\n",
    "extra cheese 2.00, \\\n",
    "mushrooms 1.50 \\\n",
    "sausage 3.00 \\\n",
    "canadian bacon 3.50 \\\n",
    "AI sauce 1.50 \\\n",
    "peppers 1.00 \\\n",
    "Drinks: \\\n",
    "coke 3.00, 2.00, 1.00 \\\n",
    "sprite 3.00, 2.00, 1.00 \\\n",
    "bottled water 5.00 \\\n",
    "\"\"\"} ]  # accumulate messages\n",
    "\n",
    "# Text input widget\n",
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
    "\n",
    "# Button widget\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "\n",
    "# Interactive conversation panel\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's collect the order and output to a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  context.copy()\n",
    "messages.append(\n",
    "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
    " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},    \n",
    ")\n",
    " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},    \n",
    "\n",
    "response = get_completion_from_messages(messages, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Example: Build an End-to-End System\n",
    "\n",
    "In this section, we puts together everything we learned before to create an end-to-end system. This example requires the products.json and products.py file. You need to put them in the same folder as this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-4o\", temperature=0, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System of chained prompts for processing the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = client.moderations.create(input=user_input)\n",
    "    moderation_output = response.results[0]\n",
    "\n",
    "    if moderation_output.flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = products.find_category_and_product_only(user_input, products.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = products.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = products.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \\\n",
    "    Respond in a friendly and helpful tone, with concise answers. \\\n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = client.moderations.create(input=final_response)\n",
    "    moderation_output = response.results[0]\n",
    "\n",
    "    if moderation_output.flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that collects user and assistant messages over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(debug=False):\n",
    "    user_input = inp.value_input\n",
    "    if debug: print(f\"User Input = {user_input}\")\n",
    "    if user_input == \"\":\n",
    "        return\n",
    "    inp.value = ''\n",
    "    global context\n",
    "    #response, context = process_user_message(user_input, context, utils.get_products_and_category(),debug=True)\n",
    "    response, context = process_user_message(user_input, context, debug=False)\n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(user_input, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600)))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with the chatbot!\n",
    "Note that the system message includes detailed instructions about what the OrderBot should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"You are Service Assistant\"} ]  \n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "button_conversation = pn.widgets.Button(name=\"Service Assistant\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Handling Responses\n",
    "\n",
    "Once we send a prompt to the OpenAI API and receive a response, it's important to know how to handle and parse this response effectively. The response from the API contains a wealth of information, and we need to extract the relevant parts for our use.\n",
    "\n",
    "## Structure of the Response\n",
    "\n",
    "The response from the OpenAI API is a JSON object containing several fields. The most important field for us is `choices`, which is a list containing the model's completions. Each item in `choices` has a `message` field with the `content` field holding the actual text of the completion.\n",
    "\n",
    "Here is a simplified structure of the response:\n",
    "\n",
    "```plaintext\n",
    "{\n",
    "    'id': 'chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve',\n",
    "    'object': 'chat.completion',\n",
    "    'created': 1677649420,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'usage': {'prompt_tokens': 56, 'completion_tokens': 31, 'total_tokens': 87},\n",
    "    'choices': [\n",
    "        {\n",
    "            'message': {\n",
    "                'role': 'assistant',\n",
    "                'content': 'The translated text is: \"Bonjour, comment ça va?\"',\n",
    "            },\n",
    "            'finish_reason': 'stop',\n",
    "            'index': 0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Extracting the Completion\n",
    "\n",
    "We can extract the completion text from the response by accessing `response.choices[0].message['content']`. Let's demonstrate this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entire_completion(prompt, model=\"gpt-4o\"): # this function will get the entire response object\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response # Return the entire response object\n",
    "\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"Translate the following English text to French: 'Good Morning!'\"\n",
    "\n",
    "# Call the get_completion function with the prompt\n",
    "response = get_entire_completion(prompt)\n",
    "\n",
    "# Display the full response\n",
    "print(\"Full Response:\", response)\n",
    "\n",
    "# Extract and display the completion text from the response\n",
    "completion_text = response.choices[0].message.content\n",
    "print(\"Extracted Completion:\", completion_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "\n",
    "When working with the OpenAI API and GPT-3.5 Turbo, it's essential to follow best practices to ensure efficient, secure, and responsible usage. Below are some best practices to keep in mind:\n",
    "\n",
    "## 1. **Security and Privacy**\n",
    "   - **API Keys:** Keep your API keys secure and never expose them in public repositories, forums, or other public spaces.\n",
    "   - **User Data:** Be mindful of user privacy and avoid sending sensitive or personally identifiable information to the API.\n",
    "\n",
    "## 2. **Prompt Engineering**\n",
    "   - **Clarity and Context:** Craft clear and concise prompts with sufficient context to guide the model towards generating accurate and relevant responses.\n",
    "   - **Experimentation:** Experiment with different prompt structures, instructions, and parameters to achieve the desired output.\n",
    "\n",
    "## 3. **Response Handling**\n",
    "   - **Error Handling:** Implement robust error handling to manage potential issues that might arise during API requests.\n",
    "   - **Extraction of Information:** Understand the structure of the API response and extract the relevant information efficiently.\n",
    "\n",
    "## 4. **Resource Management**\n",
    "   - **Token Limitations:** Be aware of the model’s maximum token limit and ensure that the input and output tokens do not exceed this limit.\n",
    "   - **Usage Monitoring:** Monitor your API usage to avoid reaching rate limits and to manage costs effectively.\n",
    "\n",
    "## 5. **Model Limitations and Bias**\n",
    "   - **Understanding Limitations:** Be aware that the model does not have a true understanding of the world and generates responses based on patterns learned during training.\n",
    "   - **Mitigating Bias:** Be vigilant about potential biases in the model’s responses and consider implementing safeguards to mitigate them.\n",
    "\n",
    "## 6. **User Interaction**\n",
    "   - **User Guidance:** Provide guidance to users on how to interact with the model and set expectations regarding the model’s capabilities and limitations.\n",
    "   - **Feedback Loop:** Implement a feedback loop to collect user feedback and continuously improve the model’s performance and user experience.\n",
    "\n",
    "## 7. **Compliance with Policies**\n",
    "   - **Use Case Review:** Review OpenAI’s use case policy to ensure that your application complies with OpenAI’s ethical guidelines and usage restrictions.\n",
    "   - **Content Moderation:** Implement content moderation to filter out inappropriate or harmful content from the model’s responses.\n",
    "\n",
    "By adhering to these best practices, you can optimize your interaction with the OpenAI API, build more robust applications, and ensure ethical and responsible usage of the technology.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
